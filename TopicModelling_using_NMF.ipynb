{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our sample corpus of text, we will use a corpus of news articles collected in 2016. These articles have been stored in a single file and formatted so that one article appears on each line. We will load these articles into a list, and also create a short snippet of text for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4551 raw text documents\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "raw_documents = []\n",
    "snippets = []\n",
    "with open( \"articles.txt\" ,\"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        text = line.strip()\n",
    "        raw_documents.append( text )\n",
    "        # keep a short snippet of up to 100 characters as a title for each article\n",
    "        snippets.append( text[0:min(len(text),100)] )\n",
    "print(\"Read %d raw text documents\" % len(raw_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Removing stopwords **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preprocessing text, a common approach is to remove non-informative stopwords. The choice of stopwords can have a considerable impact later on. We will use a custom stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword list has 350 entries\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = []\n",
    "with open( \"stopwords.txt\", \"r\" ) as fin:\n",
    "    for line in fin.readlines():\n",
    "        custom_stop_words.append( line.strip() )\n",
    "# note that we need to make it hashable\n",
    "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Term Weighting with TF-IDF ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the usefulness of the document-term matrix by giving more weight to the more \"important\" terms. The most common normalisation is term frequency–inverse document frequency (TF-IDF). In Scikit-learn, we can generate at TF-IDF weighted document-term matrix by using TfidfVectorizer in place of CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4551 X 10285 TF-IDF-normalized document-term matrix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# we can pass in the same preprocessing parameters\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 20)\n",
    "A = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 10285 distinct terms\n"
     ]
    }
   ],
   "source": [
    "# extract the resulting vocabulary\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key input parameter to NMF is the number of topics to generate k. For the moment, we will pre-specify a guessed value, for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another choice for NMF revolves around initialisation. Most commonly, NMF involves using random initialisation to populate the values in the factors W and H. Depending on the random seed that you use, you may get different results on the same dataset. Instead, using SVD-based initialisation provides more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "from sklearn import decomposition\n",
    "model = decomposition.NMF( init=\"nndsvd\", n_components=k ) \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( A )\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF produces to factor matrices as its output: W and H.\n",
    "\n",
    "The W factor contains the document membership weights relative to each of the k topics. Each row corresponds to a single document, and each column correspond to a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4551, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H factor contains the term weights relative to each of the k topics. In this case, each row corresponds to a topic, and each column corresponds to a unique term in the corpus vocabulary.",
    "For instance, for the term brexit, we see that it is strongly associated with a single topic. Again, in some cases each term can be associated with multiple topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99, 0.  , 0.  , 0.  , 0.04, 0.  , 0.  , 0.  , 0.25, 0.  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index = terms.index('brexit')\n",
    "# round to 2 decimal places for display purposes\n",
    "H[:,term_index].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Descriptors ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ranked terms from the H factor for each topic can give us an insight into the content of that topic. This is often called the topic descriptor. Let's define a function that extracts the descriptor for a specified topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get a descriptor for each topic using the top ranked terms (e.g. top 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: eu, uk, brexit, britain, european, leave, europe, vote, referendum, trade\n",
      "Topic 02: trump, clinton, donald, republican, campaign, president, hillary, cruz, sanders, presidential\n",
      "Topic 03: film, films, movie, star, director, hollywood, actor, story, drama, cinema\n",
      "Topic 04: league, season, leicester, goal, premier, united, city, liverpool, game, ball\n",
      "Topic 05: bank, banks, banking, financial, rbs, customers, shares, deutsche, barclays, lloyds\n",
      "Topic 06: health, nhs, care, patients, mental, doctors, hospital, people, services, junior\n",
      "Topic 07: album, music, band, song, pop, songs, rock, love, sound, bowie\n",
      "Topic 08: facebook, internet, online, twitter, users, google, people, media, company, amazon\n",
      "Topic 09: labour, party, corbyn, cameron, referendum, vote, voters, campaign, johnson, minister\n",
      "Topic 10: women, abortion, woman, men, cancer, female, ireland, girls, rights, northern\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Relevant Documents ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the snippets for the top-ranked documents for each topic. We'll define a function to produce this ranking also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_snippets( all_snippets, W, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
    "    # now get the snippets corresponding to the top-ranked indices\n",
    "    top_snippets = []\n",
    "    for doc_index in top_indices[0:top]:\n",
    "        top_snippets.append( all_snippets[doc_index] )\n",
    "    return top_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, for the first topic listed above, the top 10 documents are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Brexit X-men: how the prime minister’s key negotiators are coping When Boris Johnson was working for\n",
      "02. Archbishop accuses Farage of racism and 'accentuating fear for political gain' – EU referendum live \n",
      "03. Brexit could shift Europe's political centre of gravity, says Fitch A vote for Brexit in next month’\n",
      "04. EU leaders line up to insist UK will pay a high price for Brexit stance Britain and the EU appear mo\n",
      "05. Would Brexit make UK businesses less competitive? In this week’s EU referendum Q&A our panel discuss\n",
      "06. David Cameron: being in the EU gives Britain key counter-terrorism information David Cameron has sai\n",
      "07. Slovakian foreign minister: I will support any measure to stop Brexit The new Slovakian EU presidenc\n",
      "08. Brexit weekly briefing: we're going to be kept in the dark Welcome to the weekly Brexit briefing, a \n",
      "09. What would Brexit mean for housing, regeneration and central government? Housing and regeneration: T\n",
      "10. Cameron names referendum date as Gove declares for Brexit – as it happened • David Cameron has calle\n"
     ]
    }
   ],
   "source": [
    "topic_snippets = get_top_snippets( snippets, W, 0, 10 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the second topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. Donald Trump: money raised by Hillary Clinton is 'blood money' – as it happened Hillary Clinton deli\n",
      "02. Second US presidential debate – as it happened Here’s how searches for “Hillary Clinton” and “Donald\n",
      "03. Trump campaign reportedly vetting Christie, Gingrich as potential running mates – as it happened Don\n",
      "04. Donald Trump hits delegate count needed for Republican nomination – as it happened On the one hand, \n",
      "05. Trump: 'Had I been president, Capt Khan would be alive today' – as it happened Speaking with ABC New\n",
      "06. Clinton seizes on Trump tweets for day of campaigning in Florida – as it happened Donald Trump was a\n",
      "07. Melania Trump defends husband's 'boy talk' in CNN interview – as it happened Speaking to CNN’s Ander\n",
      "08. Hillary Clinton: 'I'm sick of the Sanders campaign's lies' – as it happened One of the most importan\n",
      "09. Donald Trump at the White House: Obama reports 'excellent conversation' – as it happened Are you adj\n",
      "10. Donald Trump: Hillary Clinton has 'no right to be running' – as it happened Deep antipathy to Hillar\n"
     ]
    }
   ],
   "source": [
    "topic_snippets = get_top_snippets( snippets, W, 1, 10 )\n",
    "for i, snippet in enumerate(topic_snippets):\n",
    "    print(\"%02d. %s\" % ( (i+1), snippet ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
